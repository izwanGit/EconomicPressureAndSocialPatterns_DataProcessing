# -*- coding: utf-8 -*-
"""Crime_data_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gIDCKtLS7Kttt73p4xcNC1V6DDuZPtAF

**Step 1: Import Libraries and Load Data**

In this step, we begin by importing the necessary libraries: pandas for data manipulation, MinMaxScaler for normalizing crime totals, and files for handling file uploads in Google Colab. The crime data CSV file (crime_district.csv) is uploaded manually and then loaded into a pandas DataFrame using pd.read_csv(), making the data ready for further processing.
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from google.colab import files
import io

# Upload the CSV file
uploaded = files.upload()
crime_data = pd.read_csv(io.BytesIO(uploaded['crime_district.csv']))

"""**Step 2: Data Cleaning**

The data cleaning process involves several tasks:

* Removing Duplicate Rows: Any duplicate entries are removed using drop_duplicates(), ensuring the data is clean and unique.
* Handling Missing Values: We remove rows with missing values in critical columns (date, state, district, and crimes) and fill any missing values in the crimes column with 0.
* Datetime Conversion: The date column is converted to a proper datetime format to ensure consistency and enable time-based filtering.

These actions ensure that the dataset is ready for meaningful analysis without any inconsistencies or gaps.
"""

# Remove duplicates and rows with missing critical values
crime_data = crime_data.drop_duplicates()
crime_data = crime_data.dropna(subset=['date', 'state', 'district', 'crimes'])

# Fill missing 'crimes' with 0
crime_data['crimes'].fillna(0, inplace=True)

# Convert 'date' to datetime, invalid dates become NaT
crime_data['date'] = pd.to_datetime(crime_data['date'], errors='coerce')

"""**Step 3: Filtering and Grouping Data**

Once the data is clean, we focus on filtering and grouping:

* Filtering for 2022: We narrow the data to include only records from the year 2022 by extracting the year from the date column.
* Excluding Irrelevant States: We exclude rows where the state is labeled as "Malaysia" to focus only on individual states.
* State-Level Totals: We filter for rows with district equal to "All" and type equal to "all" to obtain state-level crime totals. These totals are then grouped by state and summed to get the total crimes per state.
"""

# Filter data for the year 2022 and exclude 'Malaysia' from the 'state' column
crime_data_2022 = crime_data[crime_data['date'].dt.year == 2022]
crime_data_2022 = crime_data_2022[crime_data_2022['state'] != 'Malaysia']

# Get total crimes for each state where district is 'All' and type is 'all'
state_totals = crime_data_2022[(crime_data_2022['district'] == 'All') & (crime_data_2022['type'] == 'all')]

# Group by state and sum the crimes
state_crime_totals = state_totals.groupby('state')['crimes'].sum().reset_index()

"""**Step 4: Adjusting, Combining, and Normalizing Crime Totals**

In this step, we adjust the totals for specific regions to ensure accuracy:

* Adjusting Wilayah Persekutuan: The total for W.P. Kuala Lumpur is calculated, including Putrajaya as part of its districts.
* Adjusting Sabah: The total for Sabah is corrected by subtracting crimes related to W.P. Labuan, which is a separate district.
* Removing Duplicates for Wilayah Persekutuan: To avoid duplication, the entry for W.P. Kuala Lumpur is removed. Additionally, a combined total for Wilayah Persekutuan (including Kuala Lumpur and Putrajaya) is added as a separate row.
* Adjusting Crime Totals for Wilayah Persekutuan: The crime total for Wilayah Persekutuan is updated by adding the crime totals from W.P. Labuan, which were initially subtracted in the Sabah adjustment. This ensures the combined total is accurate.

We then normalize the crimes column using MinMaxScaler, scaling the data to a range between 0 and 1, making it easier to compare crime levels across states.
"""

# Calculate total crimes for W.P. Kuala Lumpur and Labuan
wp_kl_total = crime_data_2022[crime_data_2022['state'] == 'W.P. Kuala Lumpur']['crimes'].sum()
labuan_total = crime_data_2022[(crime_data_2022['state'] == 'Sabah') &
                                (crime_data_2022['district'] == 'W.P Labuan')]['crimes'].sum()

# Combine totals for Wilayah Persekutuan
wilayah_persekutuan_total = wp_kl_total + labuan_total
state_crime_totals.loc[state_crime_totals['state'] == 'Sabah', 'crimes'] -= labuan_total
state_crime_totals = state_crime_totals[state_crime_totals['state'] != 'W.P. Kuala Lumpur']
state_crime_totals.loc[len(state_crime_totals)] = ['Wilayah Persekutuan', wilayah_persekutuan_total]
state_crime_totals.loc[state_crime_totals['state'] == 'Wilayah Persekutuan', 'crimes'] += labuan_total

# Normalize crime data using MinMaxScaler
scaler = MinMaxScaler()
state_crime_totals['crimes_normalized'] = scaler.fit_transform(state_crime_totals[['crimes']])

"""**Step 5: Finalizing and Saving the Data**

The final crime totals are sorted by the total number of crimes in descending order for clarity. After sorting, the cleaned and processed DataFrame is saved to a new CSV file (state_crime_totals_2022.csv), and a download link is provided to the user. This step ensures that the user has access to the final dataset for further use or analysis.
"""

# Sort the state crime totals by 'crimes' in descending order and reset index
state_crime_totals = state_crime_totals.sort_values(by='crimes', ascending=False).reset_index(drop=True)

# Print the sorted data and save it to a CSV file
print(state_crime_totals)
state_crime_totals.to_csv('state_crime_totals_2022.csv', index=False)

# Download the CSV file
from google.colab import files
files.download('state_crime_totals_2022.csv')